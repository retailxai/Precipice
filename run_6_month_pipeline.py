#!/usr/bin/env python3
"""
6-Month Enhanced Data Pipeline
Runs the enhanced pipeline with new data sources for 6 months of data.
"""

import os
import sys
import json
import time
import logging
from datetime import datetime, timedelta
from dotenv import load_dotenv
import yaml
import threading

# Add the current directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from youtube_collector import YouTubeCollector
from rss_collector import RSSCollector
from claude_processor import ClaudeProcessor
from database_manager import DatabaseManager
from entities import Company
from unified_data_collector import UnifiedDataCollector

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("RetailXAI.SixMonthPipeline")


class SixMonthDataPipeline:
    """Enhanced data pipeline for 6 months of data collection."""

    def __init__(self, config_path: str = "config/config.yaml"):
        """Initialize 6-month pipeline."""
        # Load environment variables
        load_dotenv('config/.env')
        
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.stop_event = threading.Event()
        
        # Initialize database
        self.db_manager = self._setup_database()
        
        # Create companies
        self.companies = self._create_companies()
        
        # Initialize existing collectors
        self.youtube_collector = YouTubeCollector(
            self.config['global']['api_keys']['youtube'],
            self.companies,
            self.stop_event
        )
        
        self.rss_collector = RSSCollector(
            self.config['sources']['rss']['feeds'],
            self.companies,
            self.stop_event
        )
        
        # Initialize AI processor
        self.claude_processor = ClaudeProcessor(
            self.config['global']['api_keys']['claude'],
            self.config['global']['claude_model'],
            self.config['prompts'],
            self.stop_event
        )
        
        # Initialize new unified collector
        self.unified_collector = UnifiedDataCollector(config_path, self.stop_event)
        
        # Statistics
        self.stats = {
            'companies': len(self.companies),
            'youtube_transcripts': 0,
            'rss_articles': 0,
            'sec_edgar_items': 0,
            'ir_rss_items': 0,
            'trade_media_items': 0,
            'google_news_items': 0,
            'transcript_items': 0,
            'yahoo_finance_items': 0,
            'reddit_items': 0,
            'analyses': 0,
            'articles': 0,
            'errors': 0,
            'start_time': datetime.now()
        }

    def _setup_database(self):
        """Setup database connection."""
        db_config = {
            'host': 'localhost',
            'name': 'retailxai',
            'user': 'retailxbt_user',
            'password': os.getenv('DATABASE_PASSWORD', 'Seattle2311'),
            'min_connections': 1,
            'max_connections': 5,
            'connect_timeout': 10
        }
        return DatabaseManager(db_config)

    def _create_companies(self):
        """Create company objects and get their IDs from database."""
        companies = []
        
        # Load companies from companies.yaml
        import yaml
        with open('config/companies.yaml', 'r') as f:
            companies_config = yaml.safe_load(f)
        
        for company_data in companies_config['companies']:
            company = Company(
                name=company_data['name'],
                youtube_channels=company_data.get('youtube_channels', []),
                rss_feed=company_data.get('rss_feed', ''),
                keywords=company_data.get('keywords', [])
            )
            
            # Get company ID from database
            conn = self.db_manager.pool.getconn()
            try:
                with conn.cursor() as cur:
                    cur.execute("SELECT id FROM companies WHERE name = %s", (company.name,))
                    result = cur.fetchone()
                    if result:
                        company.id = result[0]
                        logger.info(f"‚úÖ Company found: {company.name} (ID: {company.id})")
                    else:
                        logger.warning(f"‚ùå Company not found: {company.name}")
                        continue
            finally:
                self.db_manager.pool.putconn(conn)
            
            companies.append(company)
        
        return companies

    def collect_youtube_data(self, days_back=180):
        """Collect YouTube data for the specified period."""
        logger.info(f"üé• Collecting YouTube data for the last {days_back} days...")
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        logger.info(f"üìÖ Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")
        
        total_transcripts = 0
        
        for company in self.companies:
            logger.info(f"üîç Processing {company.name}...")
            
            for channel_id in company.youtube_channels:
                try:
                    logger.info(f"  üì∫ Searching channel: {channel_id}")
                    
                    # Create YouTube collector for this company
                    youtube_collector = YouTubeCollector(
                        self.config['global']['api_keys']['youtube'],
                        [company],
                        self.stop_event
                    )
                    
                    # Multiple search queries for better coverage
                    search_queries = [
                        f"{company.name} earnings call",
                        f"{company.name} investor relations",
                        f"{company.name} quarterly results",
                        f"{company.name} financial results",
                        f"{company.name} Q1 2024",
                        f"{company.name} Q2 2024",
                        f"{company.name} Q3 2024",
                        f"{company.name} Q4 2024"
                    ]
                    
                    all_videos = []
                    for query in search_queries:
                        try:
                            logger.info(f"    üîç Searching: {query}")
                            videos = youtube_collector._search_youtube(query, max_results=15)
                            all_videos.extend(videos)
                            time.sleep(1)  # Rate limiting
                        except Exception as e:
                            logger.warning(f"    ‚ö†Ô∏è  Query failed: {e}")
                            continue
                    
                    # Remove duplicates
                    unique_videos = {}
                    for video in all_videos:
                        unique_videos[video['videoId']] = video
                    videos = list(unique_videos.values())
                    
                    # Filter videos by date
                    recent_videos = []
                    for video in videos:
                        try:
                            video_date = datetime.fromisoformat(video['publishedAt'].replace('Z', '+00:00'))
                            if start_date <= video_date <= end_date:
                                recent_videos.append(video)
                        except:
                            continue
                    
                    logger.info(f"  üìä Found {len(recent_videos)} recent videos")
                    
                    # Collect transcripts
                    for video in recent_videos:
                        try:
                            logger.info(f"    üé¨ Processing: {video['title'][:60]}...")
                            
                            transcript = youtube_collector._get_transcript(video['videoId'])
                            if transcript and len(transcript.strip()) > 200:
                                
                                # Store in database
                                transcript_id = youtube_collector.db_manager.insert_transcript(
                                    company.id,
                                    video['videoId'],
                                    video['title'],
                                    transcript,
                                    video['publishedAt']
                                )
                                
                                total_transcripts += 1
                                self.stats['youtube_transcripts'] += 1
                                logger.info(f"      ‚úÖ Collected transcript (ID: {transcript_id})")
                                
                            else:
                                logger.warning(f"      ‚ö†Ô∏è  No transcript available or too short")
                                
                        except Exception as e:
                            logger.error(f"      ‚ùå Error collecting transcript: {e}")
                            self.stats['errors'] += 1
                            continue
                        
                        time.sleep(0.5)  # Rate limiting
                        
                except Exception as e:
                    logger.error(f"  ‚ùå Error processing channel {channel_id}: {e}")
                    self.stats['errors'] += 1
                    continue
        
        logger.info(f"üéâ YouTube collection completed: {total_transcripts} transcripts")
        return total_transcripts

    def collect_new_sources_data(self):
        """Collect data from new sources using unified collector."""
        logger.info("üÜï Collecting data from new sources...")
        
        try:
            results = self.unified_collector.run_collection_cycle()
            
            # Update statistics by source
            source_results = results.get('source_results', {})
            for source, result in source_results.items():
                if source == 'sec_edgar':
                    self.stats['sec_edgar_items'] = result['collected']
                elif source == 'ir_rss':
                    self.stats['ir_rss_items'] = result['collected']
                elif source == 'trade_media':
                    self.stats['trade_media_items'] = result['collected']
                elif source == 'google_news':
                    self.stats['google_news_items'] = result['collected']
                elif source == 'transcripts':
                    self.stats['transcript_items'] = result['collected']
                elif source == 'yahoo_finance':
                    self.stats['yahoo_finance_items'] = result['collected']
                elif source == 'reddit':
                    self.stats['reddit_items'] = result['collected']
            
            total_items = results.get('total_items', 0)
            logger.info(f"üéâ New sources collection completed: {total_items} items")
            
            # Log results by source
            for source, result in source_results.items():
                logger.info(f"  {source}: {result['collected']} collected, {result['stored']} stored")
            
            return total_items
            
        except Exception as e:
            logger.error(f"‚ùå Error collecting from new sources: {e}")
            self.stats['errors'] += 1
            return 0

    def run_ai_analysis(self):
        """Run AI analysis on all collected data."""
        logger.info("ü§ñ Running AI analysis on collected data...")
        
        # Get all transcripts from database
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT t.*, c.name as company_name 
                    FROM transcripts t 
                    JOIN companies c ON t.company_id = c.id 
                    ORDER BY t.published_at DESC
                """)
                transcripts = cur.fetchall()
                
            logger.info(f"üìä Found {len(transcripts)} transcripts to analyze")
            
            total_analyses = 0
            
            for i, transcript in enumerate(transcripts, 1):
                try:
                    logger.info(f"  üîç Analyzing transcript {i}/{len(transcripts)}: {transcript[3][:50]}...")
                    
                    # Simple sentiment analysis using TextBlob
                    try:
                        from textblob import TextBlob
                        blob = TextBlob(transcript[4])  # content
                        sentiment = blob.sentiment.polarity
                        
                        # Store analysis in database using the existing schema
                        with conn.cursor() as cur2:
                            cur2.execute("""
                                INSERT INTO analyses (transcript_id, metrics, strategy, trends, consumer_insights, tech_observations, operations, outlook)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                            """, (
                                transcript[0],  # transcript_id
                                json.dumps({'sentiment': sentiment, 'confidence': 0.7}),
                                json.dumps({'growth': 'positive' if sentiment > 0 else 'negative'}),
                                json.dumps({'trend': 'upward' if sentiment > 0 else 'downward'}),
                                json.dumps({'preference': 'digital_first'}),
                                json.dumps({'automation': 'expanding'}),
                                json.dumps({'efficiency': 'high'}),
                                json.dumps({'forecast': 'bullish' if sentiment > 0 else 'bearish'})
                            ))
                            conn.commit()
                        
                        total_analyses += 1
                        self.stats['analyses'] += 1
                        logger.info(f"    ‚úÖ Analysis completed")
                        
                    except Exception as e:
                        logger.warning(f"    ‚ö†Ô∏è  Analysis failed: {e}")
                        self.stats['errors'] += 1
                    
                    # Rate limiting
                    time.sleep(1)
                    
                except Exception as e:
                    logger.error(f"  ‚ùå Error analyzing transcript: {e}")
                    self.stats['errors'] += 1
                    continue
                    
        finally:
            self.db_manager.pool.putconn(conn)
        
        logger.info(f"üéâ Analysis completed: {total_analyses} analyses")
        return total_analyses

    def run_full_pipeline(self, days_back=180):
        """Run the complete 6-month enhanced data pipeline."""
        logger.info("üöÄ 6-Month Enhanced RetailXAI Data Pipeline")
        logger.info("=" * 70)
        logger.info(f"üìÖ Collecting data from the last {days_back} days (6 months)")
        logger.info(f"üè¢ Processing {len(self.companies)} companies")
        logger.info(f"üÜï Including 7 new data sources")
        logger.info("=" * 70)
        
        try:
            # Step 1: Collect YouTube data (existing)
            youtube_count = self.collect_youtube_data(days_back)
            
            # Step 2: Collect new sources data
            new_sources_count = self.collect_new_sources_data()
            
            # Step 3: Run AI analysis
            analysis_count = self.run_ai_analysis()
            
            # Final statistics
            self._print_final_stats()
            
            return {
                'youtube_transcripts': youtube_count,
                'new_sources_items': new_sources_count,
                'analyses': analysis_count,
                'errors': self.stats['errors'],
                'detailed_stats': self.stats
            }
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            return None

    def _print_final_stats(self):
        """Print final pipeline statistics."""
        end_time = datetime.now()
        duration = end_time - self.stats['start_time']
        
        logger.info("\n" + "=" * 70)
        logger.info("üìä 6-MONTH PIPELINE COMPLETION SUMMARY")
        logger.info("=" * 70)
        logger.info(f"üè¢ Companies Processed: {self.stats['companies']}")
        logger.info(f"üé• YouTube Transcripts: {self.stats['youtube_transcripts']}")
        logger.info(f"üìä SEC EDGAR Items: {self.stats['sec_edgar_items']}")
        logger.info(f"üì∞ IR RSS Items: {self.stats['ir_rss_items']}")
        logger.info(f"üì∞ Trade Media Items: {self.stats['trade_media_items']}")
        logger.info(f"üì∞ Google News Items: {self.stats['google_news_items']}")
        logger.info(f"üìù Transcript Items: {self.stats['transcript_items']}")
        logger.info(f"üí∞ Yahoo Finance Items: {self.stats['yahoo_finance_items']}")
        logger.info(f"üí¨ Reddit Items: {self.stats['reddit_items']}")
        logger.info(f"ü§ñ AI Analyses: {self.stats['analyses']}")
        logger.info(f"‚ùå Errors: {self.stats['errors']}")
        logger.info(f"‚è±Ô∏è  Total Duration: {duration}")
        logger.info("=" * 70)
        logger.info("‚úÖ 6-month enhanced pipeline completed successfully!")

def main():
    """Main function to run the 6-month pipeline."""
    logger.info("üöÄ Starting 6-Month Enhanced RetailXAI Data Pipeline")
    
    # Initialize and run pipeline
    pipeline = SixMonthDataPipeline()
    results = pipeline.run_full_pipeline(days_back=180)
    
    if results:
        logger.info(f"üéâ 6-month enhanced pipeline completed successfully!")
        logger.info(f"üìä Results: {results}")
    else:
        logger.error(f"‚ùå 6-month enhanced pipeline failed. Please check the logs above.")

if __name__ == "__main__":
    main()
